# 系统设计
[TOC]

# 分布式


## 一、分布式锁

在单机场景下，可以使用语言的内置锁来实现进程同步。但是在分布式场景下，需要同步的进程可能位于不同的节点上，那么就需要使用分布式锁。

阻塞锁通常使用互斥量来实现：

- 互斥量为 0 表示有其它进程在使用锁，此时处于锁定状态；
- 互斥量为 1 表示未锁定状态。

1 和 0 可以用一个整型值表示，也可以用某个数据是否存在表示。

### 数据库的唯一索引

获得锁时向表中插入一条记录，释放锁时删除这条记录。唯一索引可以保证该记录只被插入一次，那么就可以用这个记录是否存在来判断是否处于锁定状态。

存在以下几个问题：

- 锁没有失效时间，解锁失败的话其它进程无法再获得该锁；
- 只能是非阻塞锁，插入失败直接就报错了，无法重试；
- 不可重入，已经获得锁的进程也必须重新获取锁。

### Redis 的 SETNX 指令

使用 SETNX（set if not exist）指令插入一个键值对，如果 Key 已经存在，那么会返回 False，否则插入成功并返回 True。

SETNX 指令和数据库的唯一索引类似，保证了只存在一个 Key 的键值对，那么可以用一个 Key 的键值对是否存在来判断是否存于锁定状态。

EXPIRE 指令可以为一个键值对设置一个过期时间，从而避免了数据库唯一索引实现方式中释放锁失败的问题。

### Redis 的 RedLock 算法

使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。

- 尝试从 N 个互相独立 Redis 实例获取锁；
- 计算获取锁消耗的时间，只有时间小于锁的过期时间，并且从大多数（N / 2 + 1）实例上获取了锁，才认为获取锁成功；
- 如果获取锁失败，就到每个实例上释放锁。

### Zookeeper 的有序节点

#### 1. Zookeeper 抽象模型

Zookeeper 提供了一种树形结构的命名空间，/app1/p_1 节点的父节点为 /app1。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/aefa8042-15fa-4e8b-9f50-20b282a2c624.png" width="320px"> </div><br>

#### 2. 节点类型

- 永久节点：不会因为会话结束或者超时而消失；
- 临时节点：如果会话结束或者超时就会消失；
- 有序节点：会在节点名的后面加一个数字后缀，并且是有序的，例如生成的有序节点为 /lock/node-0000000000，它的下一个有序节点则为 /lock/node-0000000001，以此类推。

#### 3. 监听器

为一个节点注册监听器，在节点状态发生改变时，会给客户端发送消息。

#### 4. 分布式锁实现

- 创建一个锁目录 /lock；
- 当一个客户端需要获取锁时，在 /lock 下创建临时的且有序的子节点；
-  客户端获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直至获得锁；
- 执行业务代码，完成后，删除对应的子节点。

#### 5. 会话超时

如果一个已经获得锁的会话超时了，因为创建的是临时节点，所以该会话对应的临时节点会被删除，其它会话就可以获得锁了。可以看到，这种实现方式不会出现数据库的唯一索引实现方式释放锁失败的问题。

#### 6. 羊群效应

一个节点未获得锁，只需要监听自己的前一个子节点，这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其它所有子节点都会收到通知（羊群效应，一只羊动起来，其它羊也会一哄而上），而我们只希望它的后一个子节点收到通知。

## 二、分布式事务

指事务的操作位于不同的节点上，需要保证事务的 ACID 特性。

例如在下单场景下，库存和订单如果不在同一个节点上，就涉及分布式事务。

分布式锁和分布式事务区别：

- 锁问题的关键在于进程操作的互斥关系，例如多个进程同时修改账户的余额，如果没有互斥关系则会导致该账户的余额不正确。
- 而事务问题的关键则在于事务涉及的一系列操作需要满足 ACID 特性，例如要满足原子性操作则需要这些操作要么都执行，要么都不执行。

### 2PC

两阶段提交（Two-phase Commit，2PC），通过引入协调者（Coordinator）来协调参与者的行为，并最终决定这些参与者是否要真正执行事务。

#### 1. 运行过程

##### 1.1 准备阶段

协调者询问参与者事务是否执行成功，参与者发回事务执行结果。询问可以看成一种投票，需要参与者都同意才能执行。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/44d33643-1004-43a3-b99a-4d688a08d0a1.png" width="550px"> </div><br>

##### 1.2 提交阶段

如果事务在每个参与者上都执行成功，事务协调者发送通知让参与者提交事务；否则，协调者发送通知让参与者回滚事务。

需要注意的是，在准备阶段，参与者执行了事务，但是还未提交。只有在提交阶段接收到协调者发来的通知后，才进行提交或者回滚。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/d2ae9932-e2b1-4191-8ee9-e573f36d3895.png" width="550px"> </div><br>

#### 2. 存在的问题

##### 2.1 同步阻塞

所有事务参与者在等待其它参与者响应的时候都处于同步阻塞等待状态，无法进行其它操作。

##### 2.2 单点问题

协调者在 2PC 中起到非常大的作用，发生故障将会造成很大影响。特别是在提交阶段发生故障，所有参与者会一直同步阻塞等待，无法完成其它操作。

##### 2.3 数据不一致

在提交阶段，如果协调者只发送了部分 Commit 消息，此时网络发生异常，那么只有部分参与者接收到 Commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。

##### 2.4 太过保守

任意一个节点失败就会导致整个事务失败，没有完善的容错机制。

### 本地消息表

本地消息表与业务数据表处于同一个数据库中，这样就能利用本地事务来保证在对这两个表的操作满足事务特性，并且使用了消息队列来保证最终一致性。

1. 在分布式事务操作的一方完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中。
2. 之后将本地消息表中的消息转发到消息队列中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发。
3. 在分布式事务操作的另一方从消息队列中读取一个消息，并执行消息中的操作。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/476329d4-e2ef-4f7b-8ac9-a52a6f784600.png" width="740px"> </div><br>

### TCC

TCC 天生适合用于需要强隔离性的分布式事务中。解决超卖问题

- **Try**：尝试执行阶段，完成所有业务可执行性的检查（保障一致性），并且预留好全部需用到的业务资源（保障隔离性）。
- **Confirm**：确认执行阶段，不进行任何业务检查，直接使用 Try 阶段准备的资源来完成业务处理。Confirm 阶段可能会重复执行，因此本阶段所执行的操作需要具备幂等性。
- **Cancel**：取消执行阶段，释放 Try 阶段预留的业务资源。Cancel 阶段可能会重复执行，也需要满足幂等性。

TCC 其实有点类似 2PC 的准备阶段和提交阶段，但 TCC 是位于用户代码层面，所以实现有较高的灵活性，可以根据需要设计资源锁定的粒度。TCC 在业务执行时只操作**预留资源**，几乎不会涉及锁和资源的争用，具有很高的性能潜力。

但是 TCC 也带来了更高的开发成本和业务侵入性，意味着有更高的开发成本和更换事务实现方案的替换成本，所以，通常我们并不会完全靠裸编码来实现 TCC，而是基于某些分布式事务中间件（譬如阿里开源的[Seata](https://seata.io/zh-cn/)）去完成，尽量减轻一些编码工作量。

### **分布式ID**

#### UUID 

UUID是基于当前时间、计数器（counter）和硬件标识（通常为无线网卡的MAC地址）等数据计算生成的。

##### 优点

- 简单，代码方便。
- 生成ID性能非常好，基本不会有性能问题。本地生成，没有网络消耗。
- 全球唯一，在遇见数据迁移，系统数据合并，或者数据库变更等情况下，可以从容应对。

##### 缺点

- 采用无意义字符串，没有排序，无法保证趋势递增。
- UUID使用字符串形式存储，数据量大时查询效率比较低
- 存储空间比较大，如果是海量数据库，就需要考虑存储量的问题。

#### 雪花算法

使用一个 64 bit 的 long 型的**数字**作为全局唯一 id，引入了时间戳，基本上保持自增的。

##### 优点

- 毫秒数在高位，自增序列在低位，整个ID都是趋势递增的。
- 不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的。
- 可以根据自身业务特性分配bit位，非常灵活。

##### 缺点

- 雪花算法在单机系统上ID是递增的，但是在分布式系统多节点的情况下，所有节点的时钟并不能保证不完全同步，所以有可能会出现不是全局递增的情况。如果系统时间被回调，或者改变，可能会造成id冲突或者重复。

#### 利用数据库的auto_increment特性

以MySQL举例，利用给字段设置auto_increment_increment和auto_increment_offset来保证ID自增，每次业务使用下列SQL读写MySQL得到ID号

##### 优点

- 非常简单，利用现有数据库系统的功能实现，成本小，有DBA专业维护。
- ID号单调自增，可以实现一些对ID有特殊要求的业务。

##### 缺点

- 强依赖DB，当DB异常时整个系统不可用，属于致命问题。配置主从复制可以尽可能的增加可用性，但是数据一致性在特殊情况下难以保证。主从切换时的不一致可能会导致重复发号。
- ID发号性能瓶颈限制在单台MySQL的读写性能
- 分表分库，数据迁移合并等比较麻烦

#### Redis的INCR

Redis是单线程的，所以也可以用生成全局唯一的ID。可以用Redis的原子操作 INCR和INCRBY来实现。

比较适合使用Redis来生成每天从0开始的流水号。比如订单号=日期+当日自增长号。可以每天在Redis中生成一个Key，使用INCR进行累加。

redis加lua脚本也可以实现twitter的snowflake算法。

##### 优点

- 不依赖于数据库，灵活方便，且性能优于数据库。
- 数字ID天然排序，对分页或者需要排序的结果很有帮助。

##### 缺点

- 如果系统中没有Redis，还需要引入新的组件，增加系统复杂度。
- 需要编码和配置的工作量比较大。

#### 京东商品订单唯一id生成方案

自增整型，定期扩展位数。为保证全局唯一和并发性，上游向队列主动生成id，下游分布式消费队列，保证了id生成的唯一性和使用的分布性。

优点：中心生成id保证全局唯一

缺点：独立服务集群，配置过重，不利于嵌入高qps的系统。

## 三、CAP

分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容忍性（P：Partition Tolerance），最多只能同时满足其中两项。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/a14268b3-b937-4ffa-a34a-4cc53071686b.jpg" width="450px"> </div><br>

### 一致性

一致性指的是**多个数据副本是否能保持一致**的特性，在一致性的条件下，系统在执行数据更新操作之后能够从一致性状态转移到另一个一致性状态。

对系统的一个数据更新成功之后，如果所有用户都能够读取到最新的值，该系统就被认为具有强一致性。

### 可用性

可用性指分布式系统在**面对各种异常时可以提供正常服务的能力**，可以用系统可用时间占总时间的比值来衡量，4 个 9 的可用性表示系统 99.99% 的时间是可用的。

在可用性条件下，要求系统提供的服务一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。

### 分区容忍性

网络分区指分布式系统中的节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信。

在分区容忍性条件下，分布式系统在遇到任何网络分区故障的时候，仍然需要能对外提供一致性和可用性的服务，除非是整个网络环境都发生了故障。

### 权衡

在分布式系统中，分区容忍性必不可少，因为需要总是假设网络是不可靠的。因此，CAP 理论实际上是要在可用性和一致性之间做权衡。

可用性和一致性往往是冲突的，很难使它们同时满足。在多个节点之间进行数据同步时，

- 为了保证一致性（CP），不能访问未同步完成的节点，也就失去了部分可用性；
- 为了保证可用性（AP），允许读取所有节点的数据，但是数据可能不一致。

## 四、BASE

BASE 是基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）三个短语的缩写。

BASE 理论是对 CAP 中一致性和可用性权衡的结果，它的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。


### 基本可用

指分布式系统在出现故障的时候，保证核心可用，允许损失部分可用性。

例如，电商在做促销时，为了保证购物系统的稳定性，部分消费者可能会被引导到一个降级的页面。

### 软状态

指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性，即允许系统不同节点的数据副本之间进行同步的过程存在时延。

### 最终一致性

最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态。

ACID 要求强一致性，通常运用在传统的数据库系统上。而 BASE 要求最终一致性，通过牺牲强一致性来达到可用性，通常运用在大型分布式系统中。

在实际的分布式场景中，不同业务单元和组件对一致性的要求是不同的，因此 ACID 和 BASE 往往会结合在一起使用。

## 五、Paxos

用于达成共识性问题，即对多个节点产生的值，该算法能保证只选出唯一一个值。

主要有三类节点：

- 提议者（Proposer）：提议一个值；
- 接受者（Acceptor）：对每个提议进行投票；
- 告知者（Learner）：被告知投票的结果，不参与投票过程。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/b988877c-0f0a-4593-916d-de2081320628.jpg"/> </div><br>

### 执行过程

规定一个提议包含两个字段：[n, v]，其中 n 为序号（具有唯一性），v 为提议值。

#### 1. Prepare 阶段

下图演示了两个 Proposer 和三个 Acceptor 的系统中运行该算法的初始过程，每个 Proposer 都会向所有 Acceptor 发送 Prepare 请求。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/1a9977e4-2f5c-49a6-aec9-f3027c9f46a7.png"/> </div><br>

当 Acceptor 接收到一个 Prepare 请求，包含的提议为 [n1, v1]，并且之前还未接收过 Prepare 请求，那么发送一个 Prepare 响应，设置当前接收到的提议为 [n1, v1]，并且保证以后不会再接受序号小于 n1 的提议。

如下图，Acceptor X 在收到 [n=2, v=8] 的 Prepare 请求时，由于之前没有接收过提议，因此就发送一个 [no previous] 的 Prepare 响应，设置当前接收到的提议为 [n=2, v=8]，并且保证以后不会再接受序号小于 2 的提议。其它的 Acceptor 类似。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/fb44307f-8e98-4ff7-a918-31dacfa564b4.jpg"/> </div><br>

如果 Acceptor 接收到一个 Prepare 请求，包含的提议为 [n2, v2]，并且之前已经接收过提议 [n1, v1]。如果 n1 \> n2，那么就丢弃该提议请求；否则，发送 Prepare 响应，该 Prepare 响应包含之前已经接收过的提议 [n1, v1]，设置当前接收到的提议为 [n2, v2]，并且保证以后不会再接受序号小于 n2 的提议。

如下图，Acceptor Z 收到 Proposer A 发来的 [n=2, v=8] 的 Prepare 请求，由于之前已经接收过 [n=4, v=5] 的提议，并且 n \> 2，因此就抛弃该提议请求；Acceptor X 收到 Proposer B 发来的 [n=4, v=5] 的 Prepare 请求，因为之前接收到的提议为 [n=2, v=8]，并且 2 \<= 4，因此就发送 [n=2, v=8] 的 Prepare 响应，设置当前接收到的提议为 [n=4, v=5]，并且保证以后不会再接受序号小于 4 的提议。Acceptor Y 类似。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/2bcc58ad-bf7f-485c-89b5-e7cafc211ce2.jpg"/> </div><br>

#### 2. Accept 阶段

当一个 Proposer 接收到超过一半 Acceptor 的 Prepare 响应时，就可以发送 Accept 请求。

Proposer A 接收到两个 Prepare 响应之后，就发送 [n=2, v=8] Accept 请求。该 Accept 请求会被所有 Acceptor 丢弃，因为此时所有 Acceptor 都保证不接受序号小于 4 的提议。

Proposer B 过后也收到了两个 Prepare 响应，因此也开始发送 Accept 请求。需要注意的是，Accept 请求的 v 需要取它收到的最大提议编号对应的 v 值，也就是 8。因此它发送 [n=4, v=8] 的 Accept 请求。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/9b838aee-0996-44a5-9b0f-3d1e3e2f5100.png"/> </div><br>

#### 3. Learn 阶段

Acceptor 接收到 Accept 请求时，如果序号大于等于该 Acceptor 承诺的最小序号，那么就发送 Learn 提议给所有的 Learner。当 Learner 发现有大多数的 Acceptor 接收了某个提议，那么该提议的提议值就被 Paxos 选择出来。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/bf667594-bb4b-4634-bf9b-0596a45415ba.jpg"/> </div><br>

### 约束条件

#### 1\. 正确性

指只有一个提议值会生效。

因为 Paxos 协议要求每个生效的提议被多数 Acceptor 接收，并且 Acceptor 不会接受两个不同的提议，因此可以保证正确性。

#### 2\. 可终止性

指最后总会有一个提议生效。

Paxos 协议能够让 Proposer 发送的提议朝着能被大多数 Acceptor 接受的那个提议靠拢，因此能够保证可终止性。

## 六、Raft

Raft 也是分布式一致性协议，主要是用来竞选主节点。

- [Raft: Understandable Distributed Consensus](http://thesecretlivesofdata.com/raft)

### 单个 Candidate 的竞选

有三种节点：Follower、Candidate 和 Leader。Leader 会周期性的发送心跳包给 Follower。每个 Follower 都设置了一个随机的竞选超时时间，一般为 150ms\~300ms，如果在这个时间内没有收到 Leader 的心跳包，就会变成 Candidate，进入竞选阶段。

- 下图展示一个分布式系统的最初阶段，此时只有 Follower 没有 Leader。Node A 等待一个随机的竞选超时时间之后，没收到 Leader 发来的心跳包，因此进入竞选阶段。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/111521118015898.gif"/> </div><br>

- 此时 Node A 发送投票请求给其它所有节点。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/111521118445538.gif"/> </div><br>

- 其它节点会对请求进行回复，如果超过一半的节点回复了，那么该 Candidate 就会变成 Leader。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/111521118483039.gif"/> </div><br>

- 之后 Leader 会周期性地发送心跳包给 Follower，Follower 接收到心跳包，会重新开始计时。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/111521118640738.gif"/> </div><br>

### 多个 Candidate 竞选

- 如果有多个 Follower 成为 Candidate，并且所获得票数相同，那么就需要重新开始投票。例如下图中 Node B 和 Node D 都获得两票，需要重新开始投票。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/111521119203347.gif"/> </div><br>

- 由于每个节点设置的随机竞选超时时间不同，因此下一次再次出现多个 Candidate 并获得同样票数的概率很低。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/111521119368714.gif"/> </div><br>

### 数据同步

- 来自客户端的修改都会被传入 Leader。注意该修改还未被提交，只是写入日志中。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/71550414107576.gif"/> </div><br>

- Leader 会把修改复制到所有 Follower。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/91550414131331.gif"/> </div><br>

- Leader 会等待大多数的 Follower 也进行了修改，然后才将修改提交。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/101550414151983.gif"/> </div><br>

- 此时 Leader 会通知的所有 Follower 让它们也提交修改，此时所有节点的值达成一致。

<div align="center"> <img src="https://cs-notes-1256109796.cos.ap-guangzhou.myqcloud.com/111550414182638.gif"/> </div><br>

# KafKa

## 架构

Kafka部分名词解释如下：

- Broker：消息中间件处理结点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群。
- Topic：一类消息，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发。
- Partition：topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。
- Segment：partition物理上由多个segment组成
- offset：每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序列号叫做offset,用于partition唯一标识一条消息.

Producer使用push(推)模式将消息发布到broker，Consumer使用pull(拉)模式从broker订阅并消费消息。

## 消费者组

Consumer Group 是 Kafka 提供的可扩展且具有容错性的消费者机制。组内必然可以有多个消费者或消费者实例（Consumer Instance），它们共享一个公共的 ID，这个 ID 被称为 Group ID。组内的所有消费者协调在一起来消费订阅主题（Subscribed Topics）的所有分区（Partition）。当然，每个分区只能由同一个消费者组内的一个 Consumer 实例来消费。

1. Consumer Group 下可以有一个或多个 Consumer 实例。
2. Group ID 是一个字符串，在一个 Kafka 集群中，它标识唯一的一个 Consumer Group。
3. 某个分区只会被一个消费者组内的一个（不多不少，必然会有一个，且只会有一个）消费者实例消费。但是这个分区可以被多个消费者组消费

### 重平衡

重平衡其实就是一个协议，它规定了如何让消费者组下的所有消费者来分配topic中的每一个分区。比如一个topic有100个分区，一个消费者组内有20个消费者，在协调者的控制下让组内每一个消费者分配到5个分区，这个分配的过程就是重平衡。

重平衡的触发条件主要有三个：

- 消费者组内成员发生变更，这个变更包括了增加和减少消费者。注意这里的减少有很大的可能是被动的，就是某个消费者崩溃退出了
- 主题的分区数发生变更，kafka目前只支持增加分区，当增加的时候就会触发重平衡
- 订阅的主题发生变化，当消费者组使用正则表达式订阅主题，而恰好又新建了对应的主题，就会触发重平衡

为什么说重平衡为人诟病呢？**因为重平衡过程中，消费者无法从kafka消费消息，这对kafka的TPS影响极大，而如果kafka集内节点较多，比如数百个，那重平衡可能会耗时极多。数分钟到数小时都有可能，而这段时间kafka基本处于不可用状态。**所以在实际环境中，应该尽量避免重平衡发生。

了解了什么是重平衡，重平衡的缺点和触发条件后，我们先来看看重平衡的三种不同策略，然后说说应该如何避免重平衡发生。

### 三种重平衡策略

kafka提供了三种重平衡分配策略，这里顺便介绍一下：

#### 随机策略

具体实现位于，package org.apache.kafka.clients.consumer.RangeAssignor。

**这种分配是基于每个主题的分区分配**，如果主题的分区不能平均分配给组内每个消费者，那么对该主题，某些消费者会被分配到额外的分区。我们来看看具体的例子。

**举例**：目前有两个消费者C0和C1，两个主题t0和t1，每个主题三个分区，分别是t0p0，t0p1，t0p2，和t1p0，t1p1，t1p2。

那么分配情况会是：

- C0：t0p0, t0p1, t1p0, t1p1
- C1：t0p2, t1p2

我来大概解释一下，range这种模式，消费者被分配的单位是基于主题的，拿上面的例子来说，是主题t0的三个分区分配给2个消费者，t1三个分区分配给消费者。于是便会出现消费者c0分配到主题t0两个分区，以及t1两个分区的情况（一个主题有三个分区，三个分区无法匹配两个消费者，势必有一个消费者分到两个分区），而非每个消费者分配两个主题各三个分区。

#### 轮询策略

具体实现位于，package org.apache.kafka.clients.consumer.RoundRobinAssignor。

**RoundRobin是基于全部主题的分区来进行分配的**，同时这种分配也是kafka默认的rebalance分区策略。还是用刚刚的例子来看，

**举例**：两个消费者C0和C1，两个主题t0和t1，每个主题三个分区，分别是t0p0，t0p1，t0p2，和t1p0，t1p1，t1p2。

由于是基于全部主题的分区，那么分配情况会是：

- C0：t0p0, t0p1, t1p1
- C1：t1p0, t0p2, t1p2

因为是基于全部主题的分区来平均分配给消费者，所以这种分配策略能更加均衡得分配分区给每一个消费者。

上面说的都是同一消费者组内消费组都订阅相同主题的情况。更复杂的情况是，同一组内的消费者订阅不同的主题，那么任然可能会导致分区不均衡的情况。

还是举例说明，有三个消费者C0，C1，C2 。三个主题t0，t1，t2，分别有1，2，3个分区 t0p0，t1p0，t1p1，t2p0，t2p1，t2p2。

其中，C0订阅t0，C1订阅t0，t1。C2订阅t0，t1，t2。最终订阅情况如下：

- C0：t0p0
- C1：t1p0
- C2：t1p1，t2p0，t2p1，t2p2

这个结果乍一看有点迷，其实可以这样理解，按照序号顺序进行循环分配，t0只有一个分区，先碰到C0就分配给它了。t1有两个分区，被C1和C2订阅，那么会循环将两个分区分配出去，最后到t2，有三个分区，却只有C2订阅，那么就将三个分区分配给C2。

#### Sticky

Sticky分配策略是最新的也是最复杂的策略，其具体实现位于package org.apache.kafka.clients.consumer.StickyAssignor。

**这种分配策略是在0.11.0才被提出来的，主要是为了一定程度解决上面提到的重平衡非要重新分配全部分区的问题。称为粘性分配策略**。

听名字就知道，主要是为了让目前的分配尽可能保持不变，只挪动尽可能少的分区来实现重平衡。

还是举例说明，有三个消费者C0，C1，C2 。三个主题t0，t1，t2，t3。每个主题各有两个分区， t0p0，t0p1，t1p0，t1p1，t2p0，t2p1，t3p0，t3p1。

现在订阅情况如下：

- C0：t0p0，t1p1，t3p0
- C1：t0p1，t2p0，t3p1
- C2：t1p0，t2p1

假设现在C1挂掉了，如果是RoundRobin分配策略，那么会变成下面这样：

- C0：t0p0，t1p0，t2p0，t3p0
- C2：t0p1，t1p1，t2p1，t3p1

就是说它会全部重新打乱，再分配，而如何使用Sticky分配策略，会变成这样：

- C0：t0p0，t1p1，t3p0，t2p0
- C2：t1p0，t2p1，t0p1，t3p1

也就是说，尽可能保留了原来的分区情况，不去改变它，在这个基础上进行均衡分配，不过这个策略目前似乎还有些bug，所以实际使用也不多。

### 避免重平衡

要说完全避免重平衡，那是不可能滴，因为你无法完全保证消费者不会故障。而消费者故障其实也是最常见的引发重平衡的地方，所以这里主要介绍如何尽力避免消费者故障。

而其他几种触发重平衡的方式，增加分区，或是增加订阅的主题，抑或是增加消费者，更多的是主动控制，这里也不多讨论。

首先要知道，如果消费者真正挂掉了，那我们是没有什么办法的，但实际中，会有一些情况，会让kafka错误地认为一个正常的消费者已经挂掉了，我们要的就是避免这样的情况出现。

当然要避免，那首先要知道哪些情况会出现错误判断挂掉的情况。在分布式系统中，通常是通过心跳来维持分布式系统的，由于网络问题你不清楚 没接收到心跳，是因为对方真正挂了还是只是因为网络堵塞。所以一般会约定一个时间，超时即判定对方挂了。**而在kafka消费者场景中，session.timout.ms参数就是规定这个超时时间是多少**。

还有一个参数，**heartbeat.interval.ms，这个参数控制发送心跳的频率**，频率越高越不容易被误判，但也会消耗更多资源。

此外，还有最后一个参数，**max.poll.interval.ms**，我们都知道**消费者**poll数据后，需要一些处理，再进行拉取。如果两次**拉取时间间隔**超过这个参数设置的值，那么消费者就会被踢出消费者组。也就是说，拉取，然后处理，这个处理的时间不能超过max.poll.interval.ms这个参数的值。这个参数的默认值是5分钟，而如果消费者接收到数据后会执行耗时的操作，则应该将其设置得大一些。

小结一下，其实主要就是三个参数，session.timout.ms控制**心跳超时时间**，heartbeat.interval.ms控制**心跳发送频率**，以及max.poll.interval.ms控制poll的间隔。这里给出一个相对较为合理的配置，如下：

- session.timout.ms：设置为6s  心跳超时时间
- heartbeat.interval.ms：设置2s  心跳发送频率
- max.poll.interval.ms：推荐为消费者处理消息最长耗时再加1分钟  消费者两次拉取时间间隔

## 备份

### 副本

本质就是一个只能追加写消息的提交日志。一个分区能够配置很多副本，他们都保存有相同的消息序列，这些副本分散保存在不同的 Broker 上，从而能够对抗部分 Broker 宕机带来的数据不可用。

在实际生产环境中，每台 Broker 都可能保存有各个主题下不同分区的不同副本，因此，单个 Broker 上存有成百上千个副本的现象是非常正常的。

### 副本角色

领导者副本（Leader Replica）和追随者副本（Follower Replica）。每个分区在创建时都要选举一个副本，称为领导者副本，其余的副本自动称为追随者副本。

所有的读写请求都必须发往领导者副本所在的 Broker，由该 Broker 负责处理。追随者副本不处理客户端请求，它唯一的任务就是从领导者副本异步拉取消息，并写入到自己的提交日志中，从而实现与领导者副本的同步。

**好处**

1. **方便实现“Read-your-writes”**。所谓 Read-your-writes，顾名思义就是，当你使用生产者 API 向 Kafka 成功写入消息后，马上使用消费者 API 去读取刚才生产的消息。举个例子，比如你平时发微博时，你发完一条微博，肯定是希望能立即看到的，这就是典型的 Read-your-writes 场景。如果允许追随者副本对外提供服务，由于副本同步是异步的，因此有可能出现追随者副本还没有从领导者副本那里拉取到最新的消息，从而使得客户端看不到最新写入的消息。
2. **方便实现单调读（Monotonic Reads）**。什么是单调读呢？就是对于一个消费者用户而言，在多次消费消息时，它不会看到某条消息一会儿存在一会儿不存在。如果允许追随者副本提供读服务，那么假设当前有 2 个追随者副本 F1 和 F2，它们异步地拉取领导者副本数据。倘若 F1 拉取了 Leader 的最新消息而 F2 还未及时拉取，那么，此时如果有一个消费者先从 F1 读取消息之后又从 F2 拉取消息，它可能会看到这样的现象：第一次消费时看到的最新消息在第二次消费时不见了，这就不是单调读一致性。但是，如果所有的读请求都是由 Leader 来处理，那么 Kafka 就很容易实现单调读一致性。

### 副本如何进行数据同步

追随者副本不提供服务，只是定期地异步拉取领导者副本中的数据而已。既然是异步的，就存在着不可能与 Leader 实时同步的风险。

ISR 副本集合。ISR 中的副本都是与 Leader 同步的副本。能够进入到 ISR 的追随者副本要满足一定的条件。这个标准就是 Broker 端参数 replica.lag.time.max.ms 参数值。这个参数的含义是 Follower 副本能够落后 Leader 副本的最长时间间隔，当前默认值是 10 秒。

Follower 副本唯一的工作就是不断地从 Leader 副本拉取消息，然后写入到自己的提交日志中。如果这个同步过程的速度持续慢于 Leader 副本的消息写入速度，那么在 replica.lag.time.max.ms 时间后，此 Follower 副本就会被认为是与 Leader 副本不同步的，因此不能再放入 ISR 中。此时，Kafka 会自动收缩 ISR 集合，将该副本“踢出”ISR。倘若该副本后面慢慢地追上了 Leader 的进度，那么它是能够重新被加回 ISR 的。这也表明，ISR 是一个动态调整的集合，而非静态不变的。

#### Unclean 领导者选举（Unclean Leader Election）

ISR 为空。**Kafka 把所有不在 ISR 中的存活副本都称为非同步副本**。通常来说，非同步副本落后 Leader 太多，因此，如果选择这些副本作为新 Leader，就可能出现数据的丢失。毕竟，这些副本中保存的消息远远落后于老 Leader 中的消息。在 Kafka 中，选举这种副本的过程称为 Unclean 领导者选举。**Broker 端参数 unclean.leader.election.enable 控制是否允许 Unclean 领导者选举**。

开启 Unclean 领导者选举可能会造成数据丢失，但好处是，它使得分区 Leader 副本一直存在，不至于停止对外提供服务，因此提升了高可用性。反之，禁止 Unclean 领导者选举的好处在于维护了数据的一致性，避免了消息丢失，但牺牲了高可用性。

如果你听说过 CAP 理论的话，你一定知道，一个分布式系统通常只能同时满足一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）中的两个。显然，在这个问题上，Kafka 赋予你选择 C 或 A 的权利。你可以根据你的实际业务场景决定是否开启 Unclean 领导者选举。不过，我强烈建议你不要开启它，毕竟我们还可以通过其他的方式来提升高可用性。如果为了这点儿高可用性的改善，牺牲了数据一致性，那就非常不值当了。

#### 高水位的作用

在 Kafka 中，高水位的作用主要有 2 个。

1. 定义消息可见性，即用来标识分区下的哪些消息是可以被消费者消费的。
2. 帮助 Kafka 完成副本同步。

在分区高水位以下的消息被认为是已提交消息，反之就是未提交消息。消费者只能消费已提交消息。

日志末端位移 LEO。它表示副本写入下一条消息的位移值。**一个副本对象的高水位值不会大于 LEO 值。**Kafka 所有副本都有对应的高水位和 LEO 值，Kafka 使用 Leader 副本的高水位来定义所在分区的高水位。换句话说，分区的高水位就是其 Leader 副本的高水位。

#### 副本同步机制解析

保存了某分区的 Leader 副本的Broker也会保存和所有 Follower 副本的 LEO 值，Kafka 把这个 Broker 上保存的这些 Follower 副本称为**远程副本**

远程副本的主要作用是，帮助 Leader 副本确定其高水位，也就是分区高水位。因为高水位的计算是：currentHW = max{currentHW, min（LEO-1, LEO-2, ……，LEO-n）}

1. Leader 副本接收到生产者发送的消息，写入了本地磁盘后 ，会更新其LEO 值。
2. Follower 从 Leader 拉取到了消息，写入了本地磁盘后 ，也更新 LEO 。
3. 在新一轮的拉取请求中，Follower 副本会通过自己更新的LEO告诉Leader副本从哪个位移处开始拉取。Leader 副本会使用这个位移值来更新远程副本的LEO。
4. Leader副本通过和远程副本中的LEO最小值比较，更新自己的高水位值。
5. Follower副本会比较自己的LEO值和接收到的Leader副本发来的高水位值，并取较小值更新自己的高水位值。

#### Leader Epoch

Leader Epoch，是 Leader 版本。它由两部分数据组成。

1. Epoch。一个单调增加的版本号。每当副本领导权发生变更时，都会增加该版本号。小版本号的 Leader 被认为是过期 Leader，不能再行使 Leader 权力。
2. 起始位移（Start Offset）。Leader 副本在该 Epoch 值上写入的首条消息的位移。

Kafka Broker 会在内存中为每个分区都缓存 Leader Epoch 数据，同时它还会定期地将这些信息持久化到一个 checkpoint 文件中。当 Leader 副本写入消息到磁盘时，Broker 会尝试更新这部分缓存。如果该 Leader 是首次写入消息，那么 Broker 会向缓存中增加一个 Leader Epoch 条目，否则就不做更新。这样，每次有 Leader 变更时，新的 Leader 副本会查询这部分缓存，取出对应的 Leader Epoch 的起始位移，以避免数据丢失和不一致的情况。

生产者程序向 Leader 副本 发送了两条消息，全部写入成功。而且 Leader 副本的高水位也已经更新了，但 Follower 副本高水位还未更新。**Follower 端高水位的更新与 Leader 端有时间错配。**

倘若此时副本 B 所在的 Broker 宕机，当它重启回来后，副本 B 会执行日志截断操作，将 LEO 值调整为之前的高水位值。此时副本 B 的底层磁盘文件中只保存有 1 条消息。

当执行完截断操作后，副本 B 开始从 A 拉取消息，执行正常的消息同步。如果就在这个节骨眼上，副本 A 所在的 Broker 宕机了，那么 Kafka 就别无选择，只能让副本 B 成为新的 Leader，此时，当 A 回来后，需要执行相同的日志截断操作，即将高水位调整为与 B 相同的值。这样操作之后，那条消息就从这两个副本中被永远地抹掉了。

引用 Leader Epoch 机制后，Follower 副本 B 重启回来后，需要向 A 发送一个特殊的请求去获取 Leader 的 LEO 值。如果B 发现该 LEO 值不比它自己的 LEO 值小，而且缓存中也没有保存任何起始位移值 > LEO值 的 Epoch 条目，因此 B 无需执行任何日志截断操作。这是对高水位机制的一个明显改进，即副本是否执行日志截断不再依赖于高水位进行判断。

现在，副本 A 宕机了，B 成为 Leader。同样地，当 A 重启回来后，执行与 B 相同的逻辑判断，发现也不用执行日志截断。消息在两个副本中均得到保留。后面当生产者程序向 B 写入新消息时，副本 B 所在的 Broker 缓存中，会生成新的 Leader Epoch 条目：[Epoch=1, Offset=2]。之后，副本 B 会使用这个条目帮助判断后续是否执行日志截断操作。这样，通过 Leader Epoch 机制，Kafka 完美地规避了这种数据丢失场景。

### 如何保证消息的顺序消费

Kafka 分布式的单位是 partition，同一个 partition 用一个 write ahead log 组织，所以可以保证 FIFO 的顺序。不同 partition 之间不能保证顺序。但是绝大多数用户都可以通过 message key 来定义，因为同一个 key 的 message 可以保证只发送到同一个 partition。



Kafka 中发送 1 条消息的时候，可以指定(topic, partition, key) 3 个参数。partiton 和 key 是可选的。如果你指定了 partition，那就是所有消息发往同 1个 partition，就是有序的。并且在消费端，Kafka 保证，1 个 partition 只能被1 个 consumer 消费。或者你指定 key（ 比如 order id），具有同 1 个 key 的所有消息，会发往同 1 个 partition。

### Kafka中如何找到offset对应的Message消息

1. kafka消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。
2. Topic是逻辑上的概念，而Partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数据。
3. Producer生产的数据会不断地追加到log文件的末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。

### 消息积压



### 消息丢失

当 Kafka 的若干个 Broker 成功地接收到一条消息并写入到日志文件后，这条消息已成功提交，并且这 N 个 Broker 中至少有 1 个存活。满足二者 Kafka 才能保证消息不丢失。

#### 生产者程序丢失数据

Producer 永远要使用带有回调通知的发送 API，也就是说不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)。如果是因为网络抖动，让 Producer 重试；如果消息不合格，调整消息格式。

#### 消费者程序丢失数据

1. Consumer 程序有个“位移”的概念，表示的是这个 Consumer 当前消费到的 Topic 分区的位置。

   维持先消费消息（阅读），再更新位移（书签）的顺序即可。这样就能最大限度地保证消息不丢失。可能带来的问题是**消息的重复处理**。

2. Consumer 程序从 Kafka 获取到消息后开启了多个线程异步处理消息，而 Consumer 程序自动地向前更新位移。假如其中某个线程运行失败了，它负责的消息没有被成功处理，但位移已经被更新了，因此这条消息对于 Consumer 而言实际上是丢失了。

   这里的关键在于 Consumer 自动提交位移，没有真正地确认消息是否真的被消费就“盲目”地更新了位移。

   解决方案：如果是多线程异步处理消费消息，Consumer 程序不要开启自动提交位移，而是要应用程序手动提交位移。避免无消费消息丢失很简单，但极易出现消息重复消费。

#### 最佳实践

1. 不要使用 producer.send(msg)，而要使用 producer.send(msg, callback)。记住，一定要使用带有回调通知的 send 方法。
2. 设置 acks = all。acks 是 Producer 的一个参数，代表了你对“已提交”消息的定义。如果设置成 all，则表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”。这是最高等级的“已提交”定义。
3. 设置 retries 为一个较大的值。这里的 retries 同样是 Producer 的参数，对应前面提到的 Producer 自动重试。当出现网络的瞬时抖动时，消息发送可能会失败，此时配置了 retries > 0 的 Producer 能够自动重试消息发送，避免消息丢失。
4. 设置 unclean.leader.election.enable = false。这是 Broker 端的参数，它控制的是哪些 Broker 有资格竞选分区的 Leader。如果一个 Broker 落后原先的 Leader 太多，那么它一旦成为新的 Leader，必然会造成消息的丢失。故一般都要将该参数设置成 false，即不允许这种情况的发生。
5. 设置 replication.factor >= 3。这也是 Broker 端的参数。其实这里想表述的是，最好将消息多保存几份，毕竟目前防止消息丢失的主要机制就是冗余。
6. 设置 min.insync.replicas > 1。这依然是 Broker 端参数，控制的是消息至少要被写入到多少个副本才算是“已提交”。设置成大于 1 可以提升消息持久性。在实际环境中千万不要使用默认值 1。
7. 确保 replication.factor > min.insync.replicas。如果两者相等，那么只要有一个副本挂机，整个分区就无法正常工作了。我们不仅要改善消息的持久性，防止数据丢失，还要在不降低可用性的基础上完成。推荐设置成 replication.factor = min.insync.replicas + 1。
8. 确保消息消费完成再提交。Consumer 端有个参数 enable.auto.commit，最好把它设置成 false，并采用手动提交位移的方式。就像前面说的，这对于单 Consumer 多线程处理的场景而言是至关重要的。

### 重复消费

Kafka 通过幂等性和事务两种机制来实现精确一次的交付可靠性保障。

#### 单分区幂等性 Producer

指定 Producer 幂等性的方法很简单，仅需要设置一个参数即可，即 props.put(“enable.idempotence”, ture)，或 props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG， true)。enable.idempotence 被设置成 true 后，Producer 自动升级成幂等性 Producer。Kafka 自动帮你做消息的重复去重。

底层具体的原理很简单，就是经典的用空间去换时间的优化思路，即在 Broker 端多保存一些字段。当 Producer 发送了具有相同字段值的消息后，Broker 能够知道这些消息已经重复了，于是可以在后台把它们“丢弃”掉。

首先，它只能保证单分区上的幂等性，即一个幂等性 Producer 能够保证某个主题的一个分区上不出现重复消息，它无法实现多个分区的幂等性。其次，它只能实现单会话上的幂等性，不能实现跨会话的幂等性。这里的会话，你可以理解为 Producer 进程的一次运行。当你重启了 Producer 进程之后，这种幂等性保证就丧失了。

#### Kafka 事务

事务型 Producer 能够保证将消息原子性地写入到多个分区中。这批消息要么全部写入成功，要么全部失败。另外，事务型 Producer 也不惧进程的重启。Producer 重启回来后，Kafka 依然保证它们发送消息的精确一次处理。

设置事务型 Producer ：

1. 开启 enable.idempotence = true。
2. 设置 Producer 端参数 transactional. id。最好为其设置一个有意义的名字。

此外，还需要调用了一些事务 API，如 initTransaction、beginTransaction、commitTransaction 和 abortTransaction，它们分别对应事务的初始化、事务开始、事务提交以及事务终止。

#### 结合业务

其实还是得结合业务来思考，我这里给几个思路：

比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。

比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。

比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。

比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。

可以用redis的setnx分布式锁来实现。比如操作订单消息，可以把订单id作为key，在消费消息时，通过setnx命令设置一下，offset提交完成后，在redis中删除订单id的key。setnx命令保证同样的订单消息，只有一个能被消费，可有效保证消费的幂等性！

### 消息积压 

1. **线上有时因为发送方发送消息速度过快，或者消费方处理消息过慢，可能会导致broker积压大量未消费消息。**
   解决方案：此种情况如果积压了上百万未消费消息需要紧急处理，可以修改消费端程序，让其将收到的消息快速转发到其他topic(可以设置很多分区)，然后再启动多个消费者同时消费新主题的不同分区。如图所示：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201225204848118.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1MDc2MTgw,size_16,color_FFFFFF,t_70)

2. **由于消息数据格式变动或消费者程序有bug，导致消费者一直消费不成功，也可能导致broker积压大量未消费消息。**
   解决方案：此种情况可以将这些消费不成功的消息转发到其它队列里去(类似死信队列)，后面再慢慢分析死信队列里的消息处理问题。

# ES

## 介绍

京东健康主数据中心业务中，由于内部上下游系统的依赖，医生医院患者信息等数据查询的调用量都非常大，造成了读多写少的情况。

我们把订单数据存储在MySQL中，但显然只通过DB来支撑大量的查询是不可取的。同时对于一些复杂的查询，MySQL支持得不够友好，所以订单中心系统使用了Elasticsearch来承载订单查询的主要压力。

先设想一个关于搜索的场景，假设我们要搜索一首诗句内容中带“头痛”字的病情描述、处方。

name	content	doctor	patient

用 传统关系型数据库和 ES 实现会有什么差别？

如果用像 MySQL 这样的 RDBMS 来存储古诗的话，我们应该会去使用这样的 SQL 去查询

```sql
select name from poems where content like "%前%";
```

这种我们称为顺序扫描法，需要遍历所有的记录进行匹配。

不但效率低，而且不符合我们搜索时的期望，比如我们在搜索“ABCD"这样的关键词时，通常还希望看到"A","AB","CD",“ABC”的搜索结果。

于是乎就有了专业的搜索引擎，比如我们今天的主角 -- ES。

Elasticsearch作为一款功能强大的分布式搜索引擎，支持近实时的存储、搜索数据

### ES 集群架构演进之路

**节点副本调优阶段**

ES的性能跟硬件资源有很大关系，当ES集群单独部署到物理机器上时，集群内部的节点并不是独占整台物理机资源，在集群运行的时候同一物理机上的节点仍会出现资源抢占的问题。所以在这种情况下，为了让ES单个节点能够使用最大程度的机器资源，采用每个ES节点部署在单独一台物理机上方式。

如果单个节点出现瓶颈了呢？我们应该怎么再优化呢？

ES查询的原理，当请求打到某号分片的时候，如果没有指定分片类型（Preference参数）查询，请求会负载到对应分片号的各个节点上。而集群默认副本配置是一主一副，针对此情况，我们想到了扩容副本的方式，由默认的一主一副变为一主二副，同时增加相应物理机。

如图，整个架设方式通过VIP*（我们知道一般的IP地址是和物理网卡绑定的，而VIP相反，是不与实际网卡绑定的的IP地址。当外网的上的一个机器，通过域名访问某公司内网资源时，内网的DNS服务器会把域名解析到一个VIP上。当外网主机经过域名解析得到这个VIP后，就将数据包发往这个VIP。但是在内网中，这个VIP是不与具体的设备相连接的，所以外网发过来的目的地址是VIP的IP数据包，究竟会到哪台机器呢？*

*其实，这个在内网的过程是，通过ARP协议来完成的。也就是说这个VIP可以映射到的MAC地址是可以控制的。VIP在内网中被动态的映射到不同的MAC地址上，也就是映射到不同的机器设备上，那么就可以起到负载均衡的效果啦。）*来负载均衡外部请求：

整个集群有一套主分片，二套副分片（一主二副），从网关节点转发过来的请求，会在打到数据节点之前通过轮询的方式进行均衡。集群增加一套副本并扩容机器的方式，增加了集群吞吐量，从而提升了整个集群查询性能。

当然分片数量和分片副本数量并不是越多越好，在此阶段，我们对选择适当的分片数量做了进一步探索。分片数可以理解为MySQL中的分库分表，而当前订单中心ES查询主要分为两类：单ID查询以及分页查询。

分片数越大，集群横向扩容规模也更大，根据分片路由的单ID查询吞吐量也能大大提升，但聚合的分页查询性能则将降低；分片数越小，集群横向扩容规模也更小，单ID的查询性能也会下降，但分页查询的性能将会提升。

所以如何均衡分片数量和现有查询业务，我们做了很多次调整压测，最终选择了集群性能较好的分片数。

**4、主从集群调整阶段**

到此，订单中心的ES集群已经初具规模，但由于订单中心业务时效性要求高，对ES查询稳定性要求也高，如果集群中有节点发生异常，查询服务会受到影响，从而影响到整个订单生产流程。很明显这种异常情况是致命的，所以为了应对这种情况，我们初步设想是增加一个备用集群，当主集群发生异常时，可以实时的将查询流量降级到备用集群。

那备用集群应该怎么来搭？主备之间数据如何同步？备用集群应该存储什么样的数据？

考虑到ES集群暂时没有很好的主备方案，同时为了更好地控制ES数据写入，我们采用业务双写的方式来搭设主备集群。每次业务操作需要写入ES数据时，同步写入主集群数据，然后异步写入备集群数据。同时由于大部分ES查询的流量都来源于近几天的订单，且订单中心数据库数据已有一套归档机制，将指定天数之前已经关闭的订单转移到历史订单库。

所以归档机制中增加删除备集群文档的逻辑，让新搭建的备集群存储的订单数据与订单中心线上数据库中的数据量保持一致。同时使用ZK在查询服务中做了流量控制开关，保证查询流量能够实时降级到备集群。在此，订单中心主从集群完成，ES查询服务稳定性大大提升。

**5、现今：实时互备双集群阶段**

期间由于主集群ES版本是较低的1.7，而现今ES稳定版本都已经迭代到6.x，新版本的ES不仅性能方面优化很大，更提供了一些新的好用的功能，所以我们对主集群进行了一次版本升级，直接从原来的1.7升级到6.x版本。

集群升级的过程繁琐而漫长，不但需要保证线上业务无任何影响，平滑无感知升级，同时由于ES集群暂不支持从1.7到6.x跨越多个版本的数据迁移，所以需要通过重建索引的方式来升级主集群，具体升级过程就不在此赘述了。

主集群升级的时候必不可免地会发生不可用的情况，但对于订单中心ES查询服务，这种情况是不允许的。所以在升级的阶段中，备集群暂时顶上充当主集群，来支撑所有的线上ES查询，保证升级过程不影响正常线上服务。同时针对于线上业务，我们对两个集群做了重新的规划定义，承担的线上查询流量也做了重新的划分。

备集群存储的是线上近几天的热点数据，数据规模远小于主集群，大约是主集群文档数的十分之一。集群数据量小，在相同的集群部署规模下，备集群的性能要优于主集群。

然而在线上真实场景中，线上大部分查询流量也来源于热点数据，所以用备集群来承载这些热点数据的查询，而备集群也慢慢演变成一个热数据集群。之前的主集群存储的是全量数据，用该集群来支撑剩余较小部分的查询流量，这部分查询主要是需要搜索全量订单的特殊场景查询以及订单中心系统内部查询等，而主集群也慢慢演变成一个冷数据集群。

同时备集群增加一键降级到主集群的功能，两个集群地位同等重要，但都可以各自降级到另一个集群。双写策略也优化为：假设有AB集群，正常同步方式写主（A集群）异步方式写备（B集群）。A集群发生异常时，同步写B集群（主），异步写A集群（备）。

### **ES 订单数据的同步方案**

**MySQL数据同步到ES中，大致总结可以分为两种方案：**

- 方案1：监听MySQL的Binlog，分析Binlog将数据同步到ES集群中。
- 方案2：直接通过ES API将数据写入到ES集群中。

考虑到订单系统ES服务的业务特殊性，对于订单数据的实时性较高，显然监听Binlog的方式相当于异步同步，有可能会产生较大的延时性。且方案1实质上跟方案2类似，但又引入了新的系统，维护成本也增高。所以订单中心ES采用了直接通过ES API写入订单数据的方式，该方式简洁灵活，能够很好的满足订单中心数据同步到ES的需求。

由于ES订单数据的同步采用的是在业务中写入的方式，当新建或更新文档发生异常时，如果重试势必会影响业务正常操作的响应时间。

所以每次业务操作只更新一次ES，如果发生错误或者异常，在数据库中插入一条补救任务，有Worker任务会实时地扫这些数据，以数据库订单数据为基准来再次更新ES数据。通过此种补偿机制，来保证ES数据与数据库订单数据的最终一致性。

### **遇到的一些坑**

**1、实时性要求高的查询走DB**

对于ES写入机制的有了解的同学可能会知道，新增的文档会被收集到Indexing Buffer，然后写入到文件系统缓存中，到了文件系统缓存中就可以像其他的文件一样被索引到。

然而默认情况文档从Indexing Buffer到文件系统缓存（即Refresh操作）是每秒分片自动刷新，所以这就是我们说ES是近实时搜索而非实时的原因：文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。

当前订单系统ES采用的是默认Refresh配置，故对于那些订单数据实时性比较高的业务，直接走数据库查询，保证数据的准确性。

**2、避免深分页查询**

ES集群的分页查询支持from和size参数，查询的时候，每个分片必须构造一个长度为from+size的优先队列，然后回传到网关节点，网关节点再对这些优先队列进行排序找到正确的size个文档。

假设在一个有6个主分片的索引中，from为10000，size为10，每个分片必须产生10010个结果，在网关节点中汇聚合并60060个结果，最终找到符合要求的10个文档。

由此可见，当from足够大的时候，就算不发生OOM，也会影响到CPU和带宽等，从而影响到整个集群的性能。所以应该避免深分页查询，尽量不去使用。

**3、FieldData与Doc Values**

**FieldData**

线上查询出现偶尔超时的情况，通过调试查询语句，定位到是跟排序有关系。排序在es1.x版本使用的是FieldData结构，FieldData占用的是JVM Heap内存，JVM内存是有限，对于FieldData Cache会设定一个阈值。

如果空间不足时，使用最久未使用（LRU）算法移除FieldData，同时加载新的FieldData Cache，加载的过程需要消耗系统资源，且耗时很大。所以导致这个查询的响应时间暴涨，甚至影响整个集群的性能。针对这种问题，解决方式是采用Doc Values。

**Doc Values**

Doc Values是一种列式的数据存储结构，跟FieldData很类似，但其存储位置是在Lucene文件中，即不会占用JVM Heap。随着ES版本的迭代，Doc Values比FieldData更加稳定，Doc Values在2.x起为默认设置。

### **总结**

架构的快速迭代源于业务的快速发展，正是由于近几年到家业务的高速发展，订单中心的架构也不断优化升级。而架构方案没有最好的，只有最合适的，相信再过几年，订单中心的架构又将是另一个面貌，但吞吐量更大，性能更好，稳定性更强，将是订单中心系统永远的追求。

## 查询方式

ES 有两种查询方式。本文主要介绍基于DSL语法的查询

- 使用 **Search Lite API**，它从url中获取参数
- 使用 **Json** 作为请求体，使用 ES的DSL语法来进行查询

### 全文级别查询

#### match

match 是一个标准查询，可以查询文本、数字、日期格式的数据。match 查询的一个主要用途是全文检索。

- match_phrase 与match查询不同，它是精确匹配
- multi_match 允许在做match 查询的基础上查询多个字段

### 词条级别查询

#### term

term 用于精确值的查询。

- boost参数可以提高指定字段的分数，默认值为1。
- string类型的数据在ES中可以使用text或者keyword的类型来存储。ES存储text类型的数据时会自动分词，然后建立索引。keyword不会分词，直接建立索引。如果需要对string数据进行**精确查询，应该使用keyword**的类型来存储数据。
- terms 可以指定一个字段的多个精确值。
- range 用于需要查询指定范围的内容。range 的常用参数有gte, gt  ,lte 和 lt 。ES 的date类型的数值也可以使用range查询。
- exists 返回在原始字段汇中至少有一个非空值的文档
- prefix 前缀查询

### 复合查询

#### bool

bool 查询可以合并多个过滤条件查询的结果。bool 查询可由 must, should, must not, filter 组合完成

- must 查询的内容必须出现在检索到的文档中，并且会计算文档匹配的相关度
- filter 查询的内容必须出现在检索到的文档中。与must不同，filter中的查询条件不会参与评分。filter对查询的数据有缓存功能。filter效率会比must高一些，一般，除了需要计算相关度的查询，一般使用filter
- should 至少有一个查询条件匹配，相当于 or
- must_mot 多个查询条件的相反匹配，相当于 not

## 倒排索引原理

### 什么是倒排索引

搜索的核心需求是全文检索，全文检索简单来说就是要在大量文档中找到包含某个单词出现的位置，在传统关系型数据库中，数据检索只能通过 like 来实现，例如需要在酒店数据中查询名称包含公寓的酒店，需要通过如下 sql 实现：

```
select * from hotel_table where hotel_name like '%公寓%';
```

复制代码

这种实现方式实际会存在很多问题：

- 无法使用数据库索引，需要全表扫描，性能差
- 搜索效果差，只能首尾位模糊匹配，无法实现复杂的搜索需求
- 无法得到文档与搜索条件的相关性

搜索的核心目标实际上是保证搜索的效果和性能，为了高效的实现全文检索，我们可以通过倒排索引来解决。

倒排索引是区别于正排索引的概念：

- 正排索引：是以文档对象的唯一 ID 作为索引，以文档内容作为记录的结构。
- 倒排索引：Inverted index，指的是将文档内容中的单词作为索引，将包含该词的文档 ID 作为记录的结构。

![img](https://static001.geekbang.org/infoq/4f/4fb5fc732895e42961ed9d48beecc6bd.png)

下面通过一个例子来说明下倒排索引的生成过程。

假设目前有以下两个文档内容：

> 苏州街维亚大厦 
>
> 桔子酒店苏州街店



其处理步骤如下：

1、正排索引给每个文档进行编号，作为其唯一的标识。

![img](https://static001.geekbang.org/infoq/70/702a73277848f1ac3c1b5f1a5a5ea6c2.png)



2、生成倒排索引：



a.首先要对字段的内容进行分词，分词就是将一段连续的文本按照语义拆分为多个单词，这里两个文档包含的关键词有：苏州街、维亚大厦.....

b.然后按照单词来作为索引，对应的文档 id 建立一个链表，就能构成上述的倒排索引结构。

![img](https://static001.geekbang.org/infoq/b0/b0f98f6f5bf831b88616ddbdf3f02a78.png)

有了倒排索引，能快速、灵活地实现各类搜索需求。整个搜索过程中我们不需要做任何文本的模糊匹配。



例如，如果需要在上述两个文档中查询 ***苏州街桔子*** ，可以通过分词后通过 ***苏州街*** 查到 ***1、2***，通过 ***桔子*** 查到 ***2***，然后再进行**取交取并**等操作得到最终结果。 

![img](https://static001.geekbang.org/infoq/4d/4d6d52784bf1cf27e4c21003b3a348c8.jpeg?x-oss-process=image/resize,p_80/auto-orient,1)



### 2.2. 倒排索引的结构



根据倒排索引的概念，我们可以用一个 Map 来简单描述这个结构。这个 Map 的 Key 的即是分词后的单词，这里的单词称为 Term，这一系列的 Term 组成了倒排索引的第一个部分 —— Term Dictionary (索引表，可简称为 Dictionary)。



倒排索引的另一部分为 Postings List（记录表），也对应上述 Map 结构的 Value 部分集合。

记录表由所有的 Term 对应的数据（Postings） 组成，它不仅仅为文档 id 信息，可能包含以下信息：



- 文档 id（DocId, Document Id），包含单词的所有文档唯一 id，用于去正排索引中查询原始数据。
- 词频（TF，Term Frequency），记录 Term 在每篇文档中出现的次数，用于后续相关性算分。
- 位置（Position），记录 Term 在每篇文档中的分词位置（多个），用于做词语搜索（Phrase Query）。
- 偏移（Offset），记录 Term 在每篇文档的开始和结束位置，用于高亮显示等。

![img](https://static001.geekbang.org/infoq/19/19aa1afae71f834e38bf676a7924ff36.jpeg?x-oss-process=image/resize,p_80/auto-orient,1)



------

## 3. Lucene 倒排索引的实现



全文搜索引擎在海量数据的情况下是需要存储大量的文本，所以面临以下问题：



- Dictionary 是比较大的（比如我们搜索中的一个字段可能有上千万个 Term）
- Postings 可能会占据大量的存储空间（一个 Term 多的有几百万个 doc）



因此上面说的基于 Map 的实现方式几乎是不可行的。

在海量数据背景下，倒排索引的实现直接关系到存储成本以及搜索性能。

为此，Lucene 引入了多种巧妙的数据结构和算法。其倒排索引实现拥有以下特性：



- 以较低的存储成本存储在磁盘 （索引大小大约为被索引文本的 20-30％）
- 快速读写



下面将根据倒排索引的结构，按 Posting List 和 Terms Dictionary 两部分来分析 Lucene 中的实现。



### 3.1. Posting List 实现



PostingList 包含文档 id、词频、位置等多个信息，这些数据之间本身是相对独立的，因此 Lucene 将 Postings List 被拆成三个文件存储：



- .doc 后缀文件：记录 Postings 的 docId 信息和 Term 的词频
- .pay 后缀文件：记录 Payload 信息和偏移量信息
- .pos 后缀文件：记录位置信息



基本所有的查询都会用 .doc 文件获取文档 id，且一般的查询仅需要用到 .doc 文件就足够了，只有对于近似查询等位置相关的查询则需要用位置相关数据。



三个文件整体实现差不太多，这里以.doc 文件为例分析其实现。



.doc 文件存储的是每个 Term 对应的文档 Id 和词频。每个 Term 都包含一对 TermFreqs 和 SkipData 结构。



其中 TermFreqs 存放 docId 和词频信息，SkipData 为跳表信息，用于实现 TermFreqs 内部的快速跳转。

![img](https://static001.geekbang.org/infoq/e7/e7d8a710fa5d70eb08a311919a6cca72.jpeg?x-oss-process=image/resize,p_80/auto-orient,1)



#### 3.1.1. TermFreqs



TermFreqs 存储文档号和对应的词频，它们两是一一对应的两个 int 值。Lucene 为了尽可能的压缩数据，采用的是混合存储 ，由 PackedBlock 和 VIntBlocks 两种结构组成。



**PackedBlock**

其采用 PackedInts 结构将一个 int[] 压缩打包成一个紧凑的 Block。它的压缩方式是取数组中最大值所占用的 bit 长度作为一个预算的长度，然后将数组每个元素按这个长度进行截取，以达到压缩的目的。



例如：一个包含 128 个元素的 int 数组中最大值的是 2，那么预算长度为 2 个 bit, PackedInts 的长度仅是 2 * 128 / 8 = 32 个字节，然后就可以通过 4 个 long 值存储。

![img](https://static001.geekbang.org/infoq/7e/7ee9913980300e9d5173f854f2325ed7.jpeg?x-oss-process=image/resize,p_80/auto-orient,1)

**VIntBlock**

VIntBlock 是采用 VInt 来压缩 int 值，对于绝大多数语言，int 型都占 4 个字节，不论这个数据是 1、100、1000、还是 1000,000。VInt 采用可变长的字节来表示一个整数。数值较大的数，使用较多的字节来表示，数值较少的数，使用较少的字节来表示。每个字节仅使用第 1 至第 7 位(共 7 bits)存储数据，第 8 位作为标识，表示是否需要继续读取下一个字节。



举个例子：

整数 130 为 int 类型时需要 4 个字节，转换成 VInt 后仅用 2 个字节，其中第一个字节的第 8 位为 1，标识需要继续读取第二个字节。

![img](https://static001.geekbang.org/infoq/a9/a9ea395994ac7f211c4b554539f7345d.jpeg?x-oss-process=image/resize,p_80/auto-orient,1)

根据上述两种 Block 的特点，Lucene 会每处理包含 Term 的 128 篇文档，将其对应的 DocId 数组和 TermFreq 数组分别处理为 PackedDocDeltaBlock 和 PackedFreqBlock 的 PackedInt 结构，两者组成一个 PackedBlock，最后不足 128 的文档则采用 VIntBlock 的方式来存储。

![img](https://static001.geekbang.org/infoq/01/018e897474c5790f13caed6c2ee9b88a.jpeg?x-oss-process=image/resize,p_80/auto-orient,1)



#### 3.1.2. SkipData



在搜索中存在将每个 Term 对应的 DocId 集合进行取交集的操作，即判断某个 Term 的 DocId 在另一个 Term 的 TermFreqs 中是否存在。TermFreqs 中每个 Block 中的 DocId 是有序的，可以采用顺序扫描的方式来查询，但是如果 Term 对应的 doc 特别多时搜索效率就会很低，同时由于 Block 的大小是不固定的，我们无法使用二分的方式来进行查询。因此 Lucene 为了减少扫描和比较的次数，采用了 SkipData 这个跳表结构来实现快速跳转。



**跳表**

跳表是在原有的有序链表上面增加了多级索引，通过索引来实现快速查找。

实质就是一种可以进行二分查找的有序链表。

![img](https://static001.geekbang.org/infoq/ae/aeb8f5b5e562b534b2e2f1c42720e4c6.jpeg?x-oss-process=image/resize,p_80/auto-orient,1)

**SkipData 结构**

在 TermFreqs 中每生成一个 Block 就会在 SkipData 的第 0 层生成一个节点，然后第 0 层以上每隔 N 个节点生成一个上层节点。



每个节点通过 Child 属性关联下层节点，节点内 DocSkip 属性保存 Block 的最大的 DocId 值，DocBlockFP、PosBlockFP、PayBlockFP 则表示 Block 数据对应在 .pay、.pos、.doc 文件的位置。

![img](https://static001.geekbang.org/infoq/66/6635cd475f5ee49b24825be3b000e760.jpeg?x-oss-process=image/resize,p_80/auto-orient,1)



#### 3.1.3. Posting 最终数据



Posting List 采用多个文件进行存储，最终我们可以得到每个 Term 的如下信息：



- SkipOffset：用来描述当前 term 信息在 .doc 文件中跳表信息的起始位置。
- DocStartFP：是当前 term 信息在 .doc 文件中的文档 ID 与词频信息的起始位置。
- PosStartFP：是当前 term 信息在 .pos 文件中的起始位置。
- PayStartFP：是当前 term 信息在 .pay 文件中的起始位置。



### 3.2. Term Dictionary 实现



Terms Dictionary（索引表）存储所有的 Term 数据，同时它也是 Term 与 Postings 的关系纽带，存储了每个 Term 和其对应的 Postings 文件位置指针。

![img](https://static001.geekbang.org/infoq/c0/c0626a99bb0d64f28d61058f0e8203d7.png)



#### 3.2.1. 数据存储



Terms Dictionary 通过 .tim 后缀文件存储，其内部采用 NodeBlock 对 Term 进行压缩前缀存储，处理过程会将相同前缀的的 Term 压缩为一个 NodeBlock，NodeBlock 会存储公共前缀，然后将每个 Term 的后缀以及对应 Term 的 Posting 关联信息处理为一个 Entry 保存到 Block。

![img](https://static001.geekbang.org/infoq/7b/7bedadb0a3c3d1e03a6fff38974b7fbe.png)

在上图中可以看到 Block 中还包含了 Block，这里是为了处理包含相同前缀的 Term 集合内部部分 Term 又包含了相同前缀。



举个例子，在下图中为公共前缀为 a 的 Term 集合，内部部分 Term 的又包含了相同前缀 ab，这时这部分 Term 就会处理为一个嵌套的 Block。

![img](https://static001.geekbang.org/infoq/63/63ab5bc1d6dfd34c4d926aee1432bb26.jpeg?x-oss-process=image/resize,p_80/auto-orient,1)



#### 3.2.2. 数据查找



Terms Dictionary 是按 NodeBlock 存储在.tim 文件上。当文档数量越来越多的时，Dictionary 中的 Term 也会越来越多，那查询效率必然也会逐渐变低。



因此需要一个很好的数据结构为 Dictionary 建构一个索引，这就是 Terms Index(.tip 文件存储)，Lucene 采用了 FST 这个数据结构来实现这个索引。



**FST**

FST, 全称 Finite State Transducer（有限状态转换器）。



它具备以下特点：

- 给定一个 Input 可以得到一个 output，相当于 HashMap
- 共享前缀、后缀节省空间，FST 的内存消耗要比 HashMap 少很多
- 词查找复杂度为 O(len(str))　　
- 构建后不可变更



如下图为 mon/1，thrus/4，tues/2 生成的 FST，可以看到 thrus 和 tues 共享了前缀 t 以及后缀 s。

![img](https://static001.geekbang.org/infoq/ab/ab0f6969a5064cfe52201101e49a2d58.jpeg?x-oss-process=image/resize,p_80/auto-orient,1)

根据 FST 就可以将需要搜索 Term 作为 Input，对其途径的边上的值进行累加就可以得到 output，下述为以 input 为 thrus 的读取逻辑：



- 初始状态 0
- 输入 t， FST 从 0 -> 3， output=2
- 输入 h，FST 从 3 -> 4， output=2+2=4
- 输入 r， FST 从 4 -> 5， output=4+0
- 输入 u，FST 从 5 -> 7， output=4+0
- 输入 s， FST 到达终止节点，output=4+0=4



那么 Term Dictionary 生成的 FST 对应 input 和 output 是什么呢？可能会误认为 FST 的 input 是 Dictionary 中所有的 Term，这样通过 FST 就可以找到具体一个 Term 对应的 Posting 数据。



实际上 FST 是通过 Dictionary 的每个 NodeBlock 的前缀构成，所以通过 FST 只可以直接找到这个 NodeBlock 在 .tim 文件上具体的 File Pointer, 然后还需要在 NodeBlock 中遍历 Entry 匹配后缀进行查找。



因此它在 Lucene 中充当以下功能：

1. 快速试错，即是在 FST 上找不到可以直接跳出不需要遍历整个 Dictionary，类似于 BloomFilter。
2. 快速定位 Block 的位置，通过 FST 是可以直接计算出 Block 的在文件中位置。
3. FST 也是一个 Automation(自动状态机)。这是正则表达式的一种实现方式，所以 FST 能提供正则表达式的能力。通过 FST 能够极大的提高近似查询的性能，包括通配符查询、SpanQuery、PrefixQuery 等。



### 3.3. 倒排查询逻辑



在介绍了索引表和记录表的结构后，就可以得到 Lucene 倒排索引的查询步骤：



1. 通过 Term Index 数据（.tip 文件）中的 StartFP 获取指定字段的 FST
2. 通过 FST 找到指定 Term 在 Term Dictionary（.tim 文件）可能存在的 Block
3. 将对应 Block 加载内存，遍历 Block 中的 Entry，通过后缀（Suffix）判断是否存在指定 Term
4. 存在则通过 Entry 的 TermStat 数据中各个文件的 FP 获取 Posting 数据
5. 如果需要获取 Term 对应的所有 DocId 则直接遍历 TermFreqs，如果获取指定 DocId 数据则通过 SkipData 快速跳转

![img](https://static001.geekbang.org/infoq/b8/b8e0cf56336056ff40f32a1fc14e57d1.png)



------

## 4. Lucene 数值类型处理



上述 Terms Dictionary 与 Posting List 的实现都是处理字符串类型的 Term，而对于数值类型，如果采用上述方式实现会存在以下问题：



- 数值潜在的 Term 可能会非常多，比如是浮点数，导致查询效率低
- 无法处理多维数据，比如经纬度



所以 Lucene 为了支持高效的数值类或者多维度查询，引入了 BKDTree。



### 4.1. KDTree



BKDTree 是基于 KDTree，KDTree 实现起来很像是一个二叉查找树。主要的区别是，KDTree 在不同的层使用的是不同的维度值。



下面是一个 2 维树的样例 ，其第一层以 x 为切分维度，将 x>30 的节点传递给右子树，x<30 的传递给左子树，第二层再按 y 维度切分，不断迭代到所有数据都被建立到 KD Tree 的节点上为止。

![img](https://static001.geekbang.org/infoq/62/622038aafb06bf4c4fdcdb7c6e4c5909.jpeg?x-oss-process=image/resize,p_80/auto-orient,1)



### 4.2. BKDTree



BKD 树是 KD 树和 B+ 树的组合，拥有以下特性：



- 内部 node 必须是一个完全二叉树
- 叶子节点存储点数据，降低层高度，减少磁盘 IO

![img](https://static001.geekbang.org/infoq/80/802d0f38233f1edda56855c5be205f99.jpeg?x-oss-process=image/resize,p_80/auto-orient,1)

### Lucene 和 ES

#### Lucene

Lucene 是 Elasticsearch所基于的 Java 库，它引入了按段搜索的概念。

Segment： 也叫段，类似于倒排索引，相当于一个数据集。

Commit point：提交点，记录着所有已知的段。

Lucene index： “a collection of segments plus a commit point”。由一堆 Segment 的集合加上一个提交点组成。

对于一个 Lucene index 的组成，如下图所示。

[![lucene-woYwZZ](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/lucene-woYwZZ.png)](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/lucene-woYwZZ.png)

#### Elasticsearch

一个 Elasticsearch Index 由一个或者多个 shard （分片） 组成。

[![es-shard-TjHfGE](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/es-shard-TjHfGE.png)](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/es-shard-TjHfGE.png)

而 Lucene 中的 Lucene index 相当于 ES 的一个 shard。

[![es-shard-lucene-5CpsYR](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/es-shard-lucene-5CpsYR.png)](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/es-shard-lucene-5CpsYR.png)

### 写入过程

#### 写入过程 1.0 （不完善）

1. 不断将 Document 写入到 In-memory buffer （内存缓冲区）。
2. 当满足一定条件后内存缓冲区中的 Documents 刷新到磁盘。
3. 生成新的 segment 以及一个 Commit point 提交点。
4. 这个 segment 就可以像其他 segment 一样被读取了。

画图如下：

[![es-write-1-L60ip2](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/es-write-1-L60ip2.png)](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/es-write-1-L60ip2.png)

将文件刷新到磁盘是非常耗费资源的，而且在内存缓冲区和磁盘中间存在一个高速缓存（cache），一旦文件进入到 cache 就可以像磁盘上的 segment 一样被读取了。

#### 写入过程 2.0

1. 不断将 Document 写入到 In-memory buffer （内存缓冲区）。
2. 当满足一定条件后内存缓冲区中的 Documents 刷新到 高速缓存（**cache**）。
3. 生成新的 segment ，这个 segment 还在 cache 中。
4. 这时候还没有 commit ，但是已经可以被读取了。

画图如下：

[![es-write-2-CW97xV](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/es-write-2-CW97xV.png)](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/es-write-2-CW97xV.png)

数据从 buffer 到 cache 的过程是定期每秒刷新一次。所以新写入的 Document 最慢 1 秒就可以在 cache 中被搜索到。

而 Document 从 buffer 到 cache 的过程叫做 **?refresh** 。一般是 1 秒刷新一次，不需要进行额外修改。当然，如果有修改的需要，可以参考文末的相关资料。这也就是为什么说 Elasticsearch 是**准实时**的。

使文档立即可见：

```
PUT /test/_doc/1?refresh
{"test": "test"}

// 或者
PUT /test/_doc/2?refresh=true
{"test": "test"}
```

#### Translog 事务日志

此处可以联想 Mysql 的 binlog， ES 中也存在一个 translog 用来失败恢复。

1. Document 不断写入到 In-memory buffer，此时也会追加 translog。
2. 当 buffer 中的数据每秒 refresh 到 cache 中时，translog 并没有进入到刷新到磁盘，是持续追加的。
3. translog 每隔 5s 会 fsync 到磁盘。
4. translog 会继续累加变得越来越大，当 translog 大到一定程度或者每隔一段时间，会执行 flush。

[![es-write-translog-1-L8RWwm](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/es-write-translog-1-L8RWwm.png)](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/es-write-translog-1-L8RWwm.png)

flush 操作会分为以下几步执行：

1. buffer 被清空。
2. 记录 commit point。
3. cache 内的 segment 被 fsync 刷新到磁盘。
4. translog 被删除。

[![es-write-translog-2-BWnz3P](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/es-write-translog-2-BWnz3P.png)](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/es-write-translog-2-BWnz3P.png)

值得注意的是：

1. translog 每 5s 刷新一次磁盘，所以故障重启，可能会丢失 5s 的数据。
2. translog 执行 flush 操作，默认 30 分钟一次，或者 translog 太大 也会执行。

手动执行flush：

```
POST /my-index-000001/_flush
```

### 删除和更新

segment 不可改变，所以 docment 并不能从之前的 segment 中移除或更新。

所以每次 commit， 生成 commit point 时，会有一个 .del 文件，里面会列出被删除的 document（逻辑删除）。
而查询时，获取到的结果在返回前会经过 .del 过滤。

更新时，也会标记旧的 docment 被删除，写入到 .del 文件，同时会写入一个新的文件。此时查询会查询到两个版本的数据，但在返回前会被移除掉一个。

[![es-del-1-nc0mKK](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/es-del-1-nc0mKK.png)](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/es-del-1-nc0mKK.png)

### segment 合并

每 1s 执行一次 refresh 都会将内存中的数据创建一个 segment。

segment 数目太多会带来较大的麻烦。 每一个 segment 都会消耗文件句柄、内存和cpu运行周期。更重要的是，每个搜索请求都必须轮流检查每个 segment ；所以 segment 越多，搜索也就越慢。

在 ES 后台会有一个线程进行 segment 合并。

1. refresh操作会创建新的 segment 并打开以供搜索使用。
2. 合并进程选择一小部分大小相似的 segment，并且在后台将它们合并到更大的 segment 中。这并不会中断索引和搜索。
3. 当合并结束，老的 segment 被删除 说明合并完成时的活动：
   1. 新的 segment 被刷新（flush）到了磁盘。 写入一个包含新 segment 且排除旧的和较小的 segment的新 commit point。
   2. 新的 segment 被打开用来搜索。
   3. 老的 segment 被删除。

[![segment-merge-55A1b6](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/segment-merge-55A1b6.png)](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/segment-merge-55A1b6.png)

物理删除：

在 segment merge 这块，那些被逻辑删除的 document 才会被真正的物理删除。

### 总结

主要介绍了内部写入和删除的过程，需要了解 refresh、fsync、flush、.del、segment merge 等名词的具体含义。

完整画图如下：

[![all-FMAPTS](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/all-FMAPTS.png)](https://cdn.jsdelivr.net/gh/liuzhihang/oss/pic/article/all-FMAPTS.png)